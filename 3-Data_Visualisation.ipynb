{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import watchdog.observers \n",
    "import watchdog.events \n",
    "import os\n",
    "import threading\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_file = 'dataframes.pkl'\n",
    "\n",
    "# Check if the file exists and has content, otherwise create it with an empty list\n",
    "if not os.path.exists(dataframes_file) or os.path.getsize(dataframes_file) == 0:\n",
    "    with open(dataframes_file, 'wb') as f:\n",
    "        pickle.dump([], f)\n",
    "\n",
    "# Load dataframes from file\n",
    "try:\n",
    "    with open(dataframes_file, 'rb') as f:\n",
    "        dataframes = pickle.load(f)\n",
    "except (EOFError, pickle.UnpicklingError):\n",
    "    dataframes = []\n",
    "\n",
    "processed_files = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    print(f'{i} : {df[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (721830955.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    file_path = event.src_path\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Handler(watchdog.events.PatternMatchingEventHandler):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Set the patterns for PatternMatchingEventHandler\n",
    "        watchdog.events.PatternMatchingEventHandler.__init__(self, patterns=['*.csv'],\n",
    "                                                             ignore_directories=True, case_sensitive=False)\n",
    " \n",
    "    def on_created(self, event):\n",
    "        print(\"Watchdog received created event - % s.\" % event.src_path)\n",
    "        #filename = os.path.basename(event.src_path).split('/')[-1]\n",
    "        return event.src_path\n",
    "        # Event is created, you can process it now\n",
    "            # Check if file already processed\n",
    "\n",
    "\n",
    "\n",
    "    def on_modified(self, event):\n",
    "        file_path = event.src_path\n",
    "        print(f\"Watchdog received modified event - {file_path}.\")\n",
    "        filename = os.path.basename(file_path).split('/')[-1]\n",
    "        \n",
    "        if filename not in processed_files:\n",
    "            processed_files.add(filename)\n",
    "            historical_size = -1\n",
    "            max_wait_time = 60\n",
    "            start_time = time.time()\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    current_size = os.path.getsize(file_path)\n",
    "                    if current_size == historical_size:\n",
    "                        break\n",
    "                    historical_size = current_size\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "                if time.time() - start_time > max_wait_time:\n",
    "                    print(f\"Timeout waiting for file {file_path} to stabilize.\")\n",
    "                    break\n",
    "                time.sleep(1)\n",
    "        \n",
    "            print(\"File copy has now finished or timed out.\")\n",
    "\n",
    "            global Created\n",
    "            Created = True\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    file_data = pd.read_csv(file_path)\n",
    "                    print(\"Data read from CSV:\", file_data.head())  # Debugging line\n",
    "                    \n",
    "                    if not file_data.empty:\n",
    "                        file_df = pd.DataFrame(file_data)\n",
    "                        dataframes.append([file_df, filename[:-4]])\n",
    "                        # Save dataframes to file\n",
    "                        with open(dataframes_file, 'wb') as f:\n",
    "                            pickle.dump(dataframes, f)\n",
    "                        print(f\"DataFrames updated and saved to {dataframes_file}.\")\n",
    "                    else:\n",
    "                        print(f\"No data found in CSV: {file_path}\")\n",
    "                except pd.errors.EmptyDataError:\n",
    "                    print(f\"CSV file {file_path} is empty.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "            \n",
    "            return Created\n",
    "        else:\n",
    "            print(f\"Skipping duplicate file: {filename}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    src_path = os.getcwd()\n",
    "    #r\"/home/jasmine/encryption/cryptlibs\"\n",
    "    Created = False\n",
    "    event_handler = Handler()\n",
    "\n",
    "    observer = watchdog.observers.Observer()\n",
    "    observer.schedule(event_handler, path=src_path, recursive=True)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while not Created:\n",
    "            pass\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    finally:\n",
    "        observer.stop()\n",
    "        observer.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    print(f'{i} : {df[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = {\" Data length\":[]}\n",
    "length[\" Data length\"].append(14)\n",
    "for i in range(160, 1024, 16):\n",
    "    length[\" Data length\"].append(i)\n",
    "\n",
    "data_length = pd.DataFrame(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataframes)\n",
    "for i, dataframe in enumerate(dataframes):\n",
    "    dataframes[i][0] = dataframes[i][0].rename({' Run Time': 'Run Time' + str(i), ' CPU cycles': 'CPU cycles'+str(i),  ' Throughput': 'Throughput'+str(i)}, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = data_length\n",
    "cpu_cycles = data_length\n",
    "throughputs = data_length\n",
    "\n",
    "for i, df_set in enumerate(dataframes):\n",
    "    df = df_set[0]\n",
    "    runtimes = pd.concat([runtimes, df.iloc[:, 2]], axis=1).reindex(runtimes.index)\n",
    "    cpu_cycles = pd.concat([cpu_cycles, df.iloc[:, 1]], axis=1).reindex(cpu_cycles.index)\n",
    "    throughputs = pd.concat([throughputs, df.iloc[:, 3]], axis=1).reindex(throughputs.index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorised_dfs = [runtimes, cpu_cycles, throughputs]\n",
    "for i, df in enumerate(categorised_dfs):\n",
    "    categorised_dfs[i] = categorised_dfs[i].iloc[1:]\n",
    "runtimes, cpu_cycles, throughputs = categorised_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(dataframes):\n",
    "    plt.plot(runtimes.iloc[:,0],runtimes.iloc[:,i+1], label = str(dataframes[i][1]))\n",
    "\n",
    "\n",
    "plt.xlabel(\"Data length/bytes\")\n",
    "plt.ylabel(\"Runtime/ microseconds\")\n",
    "plt.title('SHA2 v SHA3-512 Runtimes')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(dataframes):\n",
    "    plt.plot(cpu_cycles.iloc[:,0],cpu_cycles.iloc[:,i+1], label = str(dataframes[i][1]))\n",
    "\n",
    "plt.xlabel(\"Data length/bytes\")\n",
    "plt.ylabel(\"CPU Cycles/(MB/s)\")\n",
    "plt.title('SHA2 v SHA3-512 CPU Cycles')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(dataframes):\n",
    "    plt.plot(throughputs.iloc[:,0],throughputs.iloc[:,i+1], label = str(dataframes[i][1]))\n",
    "\n",
    "plt.xlabel(\"Data length/bytes\")\n",
    "plt.ylabel(\"Throughput/(B/s)\")\n",
    "plt.title('SHA2 v SHA3-512 Throughput')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
