{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import watchdog.observers \n",
    "import watchdog.events \n",
    "import os\n",
    "import threading\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_file = 'dataframes.pkl'\n",
    "# Check if the file exists and has content, otherwise create it with an empty list\n",
    "if not os.path.exists(dataframes_file) or os.path.getsize(dataframes_file) == 0:\n",
    "    with open(dataframes_file, 'wb') as f:\n",
    "        pickle.dump([], f)\n",
    "\n",
    "# Load dataframes from file\n",
    "try:\n",
    "    with open(dataframes_file, 'rb') as f:\n",
    "        dataframes = pickle.load(f)\n",
    "except (EOFError, pickle.UnpicklingError):\n",
    "    dataframes = []\n",
    "processed_files = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    print(f'{i} : {df[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Handler(watchdog.events.PatternMatchingEventHandler):\n",
    "    def __init__(self):\n",
    "\n",
    "        # Set the patterns for PatternMatchingEventHandler\n",
    "        watchdog.events.PatternMatchingEventHandler.__init__(self, patterns=['*.csv'],\n",
    "                                                             ignore_directories=True, case_sensitive=False)\n",
    " \n",
    "    def on_created(self, event):\n",
    "        print(\"Watchdog received created event - % s.\" % event.src_path)\n",
    "        #filename = os.path.basename(event.src_path).split('/')[-1]\n",
    "        return event.src_path\n",
    "        # Event is created, you can process it now\n",
    "            # Check if file already processed\n",
    "\n",
    "\n",
    "\n",
    "    def on_modified(self, event):\n",
    "        #filename = self.on_created(event)\n",
    "        file_path = event.src_path\n",
    "        print(f\"Watchdog received modified event - {file_path}.\")\n",
    "        filename = os.path.basename(file_path).split('/')[-1]\n",
    "        if filename not in processed_files:\n",
    "            processed_files.add(filename)\n",
    "            historical_size = -1\n",
    "            max_wait_time = 60  # Maximum time to wait for the file to stabilize (in seconds)\n",
    "            start_time = time.time()\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    current_size = os.path.getsize(file_path)\n",
    "                    if current_size == historical_size:\n",
    "                        break\n",
    "                    historical_size = current_size\n",
    "                except FileNotFoundError:\n",
    "                    pass\n",
    "                if time.time() - start_time > max_wait_time:\n",
    "                    print(f\"Timeout waiting for file {file_path} to stabilize.\")\n",
    "                    break\n",
    "                time.sleep(1)\n",
    "            print(\"File copy has now finished or timed out.\")\n",
    "            print(\"File copy has now finished\")\n",
    "\n",
    "            global Created\n",
    "            Created = True\n",
    "            if os.path.exists(file_path):\n",
    "                file_data = pd.read_csv(file_path)\n",
    "                file_df = pd.DataFrame(file_data)\n",
    "                dataframes.append([file_df, filename[:-4]])\n",
    "                # Save dataframes to file\n",
    "                with open(dataframes_file, 'wb') as f:\n",
    "                    pickle.dump(dataframes, f)\n",
    "            return Created  # Only return Created if the file is not processed\n",
    "        else:\n",
    "            print(f\"Skipping duplicate file: {filename}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    src_path = os.getcwd()\n",
    "    #r\"/home/jasmine/encryption/cryptlibs\"\n",
    "    Created = False\n",
    "    event_handler = Handler()\n",
    "\n",
    "    observer = watchdog.observers.Observer()\n",
    "    observer.schedule(event_handler, path=src_path, recursive=True)\n",
    "    observer.start()\n",
    "    try:\n",
    "        while not Created:\n",
    "            pass\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    finally:\n",
    "        observer.stop()\n",
    "        observer.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(dataframes):\n",
    "    print(f'{i} : {df[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = {\" Data length\":[]}\n",
    "length[\" Data length\"].append(14)\n",
    "for i in range(160, 1024, 16):\n",
    "    length[\" Data length\"].append(i)\n",
    "\n",
    "data_length = pd.DataFrame(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(dataframes)\n",
    "for i, dataframe in enumerate(dataframes):\n",
    "    dataframes[i][0] = dataframes[i][0].rename({' Run Time': 'Run Time' + str(i), ' CPU cycles': 'CPU cycles'+str(i),  ' Throughput': 'Throughput'+str(i)}, axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = data_length\n",
    "cpu_cycles = data_length\n",
    "throughputs = data_length\n",
    "\n",
    "for i, df_set in enumerate(dataframes):\n",
    "    df = df_set[0]\n",
    "    runtimes = pd.concat([runtimes, df.iloc[:, 2]], axis=1).reindex(runtimes.index)\n",
    "    cpu_cycles = pd.concat([cpu_cycles, df.iloc[:, 1]], axis=1).reindex(cpu_cycles.index)\n",
    "    throughputs = pd.concat([throughputs, df.iloc[:, 3]], axis=1).reindex(throughputs.index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorised_dfs = [runtimes, cpu_cycles, throughputs]\n",
    "for i, df in enumerate(categorised_dfs):\n",
    "    categorised_dfs[i] = categorised_dfs[i].iloc[1:]\n",
    "runtimes, cpu_cycles, throughputs = categorised_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(dataframes):\n",
    "    plt.plot(runtimes.iloc[:,0],runtimes.iloc[:,i+1], label = str(dataframes[i][1]))\n",
    "\n",
    "\n",
    "plt.xlabel(\"Data length/bytes\")\n",
    "plt.ylabel(\"Runtime/ microseconds\")\n",
    "plt.title('SHA2 v SHA3-512 Runtimes')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(dataframes):\n",
    "    plt.plot(cpu_cycles.iloc[:,0],cpu_cycles.iloc[:,i+1], label = str(dataframes[i][1]))\n",
    "\n",
    "plt.xlabel(\"Data length/bytes\")\n",
    "plt.ylabel(\"CPU Cycles/(MB/s)\")\n",
    "plt.title('SHA2 v SHA3-512 CPU Cycles')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dataframe in enumerate(dataframes):\n",
    "    plt.plot(throughputs.iloc[:,0],throughputs.iloc[:,i+1], label = str(dataframes[i][1]))\n",
    "\n",
    "plt.xlabel(\"Data length/bytes\")\n",
    "plt.ylabel(\"Throughput/(B/s)\")\n",
    "plt.title('SHA2 v SHA3-512 Throughput')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
